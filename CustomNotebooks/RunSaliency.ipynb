{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import logging\n",
    "import shlex\n",
    "import subprocess\n",
    "import sys\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import platform\n",
    "\n",
    "BIN_MAPS = {\"Darwin\": \"mac\", \"Linux\": \"linux\"}\n",
    "\n",
    "HOME_DIR = Path(\"../\").expanduser()\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    ! pip install pandas scikit-learn scikit-image statsmodels requests dash\n",
    "    ! [[ -d image-crop-analysis ]] || git clone https://github.com/twitter-research/image-crop-analysis.git\n",
    "    HOME_DIR = Path(\"./image-crop-analysis\").expanduser()\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "sys.path.append(str(HOME_DIR / \"src\"))\n",
    "bin_dir = HOME_DIR / Path(\"./bin\")\n",
    "bin_path = bin_dir / BIN_MAPS[platform.system()] / \"candidate_crops\"\n",
    "model_path = bin_dir / \"fastgaze.vxm\"\n",
    "data_dir = HOME_DIR / Path(\"./HALT_data/\")  # DATA directory\n",
    "print(\"Data directory exists:\", data_dir.exists())\n",
    "\n",
    "save_dir = HOME_DIR / Path(\"./HALT_results/\")   # SAVE directory\n",
    "print(\"Save directory exists:\", save_dir.exists())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data directory exists: True\n",
      "Save directory exists: True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Load Twitter's Saliencey + Cropping Model\n",
    "from crop_api import ImageSaliencyModel, is_symmetric, parse_output, reservoir_sampling\n",
    "\n",
    "model = ImageSaliencyModel(crop_binary_path=bin_path, crop_model_path=model_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "list(data_dir.glob(\"./*.jpg\"))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[PosixPath('../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_10.jpg'),\n",
       " PosixPath('../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_9.jpg'),\n",
       " PosixPath('../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_8.jpg'),\n",
       " PosixPath('../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_V2_5.jpg'),\n",
       " PosixPath('../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_5.jpg'),\n",
       " PosixPath('../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_4.jpg'),\n",
       " PosixPath('../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_V2_4.jpg'),\n",
       " PosixPath('../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_V2_6.jpg'),\n",
       " PosixPath('../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_6.jpg'),\n",
       " PosixPath('../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_7.jpg'),\n",
       " PosixPath('../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_V2_3.jpg'),\n",
       " PosixPath('../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_3.jpg'),\n",
       " PosixPath('../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_2.jpg'),\n",
       " PosixPath('../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_V2_2.jpg'),\n",
       " PosixPath('../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_1.jpg'),\n",
       " PosixPath('../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_V2_1.jpg')]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Do not display plots\n",
    "plt.ioff()\n",
    "\n",
    "# Run Algorithm on all Images in the Folder\n",
    "for img_path in data_dir.glob(\"*.jpg\"):\n",
    "    print(img_path)\n",
    "    model.plot_img_crops(img_path)\n",
    "\n",
    "    # Save resulting figure (which is not displayed)\n",
    "    plt.savefig(save_dir / img_path.name, bbox_inches=\"tight\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_10.jpg\n",
      "None 724 543\n",
      "../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_9.jpg\n",
      "None 1440 1071\n",
      "../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_8.jpg\n",
      "None 2121 1414\n",
      "../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_V2_5.jpg\n",
      "None 1800 1200\n",
      "../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_5.jpg\n",
      "None 811 1024\n",
      "../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_4.jpg\n",
      "None 480 300\n",
      "../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_V2_4.jpg\n",
      "None 6100 4067\n",
      "../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_V2_6.jpg\n",
      "None 1849 2589\n",
      "../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_6.jpg\n",
      "None 500 400\n",
      "../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_7.jpg\n",
      "None 1280 1024\n",
      "../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_V2_3.jpg\n",
      "None 3231 4635\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/Erick/Desktop/Academic/UofT-Undergrad/5thYear-ECE/Winter/Halt AI/image-crop-analysis/CustomNotebooks/../src/crop_api.py:297: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = plt.figure(constrained_layout=False, figsize=(fig_width, fig_height))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_3.jpg\n",
      "None 720 720\n",
      "../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_2.jpg\n",
      "None 1024 768\n",
      "../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_V2_2.jpg\n",
      "None 847 565\n",
      "../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_1.jpg\n",
      "None 800 562\n",
      "../../ImagesForTwitterAlgorithmicBias/White Hair/White Hair_V2_1.jpg\n",
      "None 800 533\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('image-crop-analysis': conda)"
  },
  "interpreter": {
   "hash": "60c6d1f01a6b86d97954e500cba66969566fa382f3ba32c579fe3b55289dceb9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}